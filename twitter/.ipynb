{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "sc = pyspark.SparkConf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as fs\n",
    "fs.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf, from_json, col\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import PipelineModel\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "# init spark here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [StructField(\"created_at\", StringType()),\n",
    "    StructField(\"message\", StringType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"Temp\", FloatType())]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder.appName(\"test\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", \n",
    "     \"true\") \\\n",
    "        .config(\"spark.jars.packages”,”org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[timestamp: timestamp, created_at: string, message: string, City: string, Temp: float]\n"
     ]
    }
   ],
   "source": [
    "kafka_topic = 'test'\n",
    "ml_path = \"\"\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .load()\\\n",
    "  .selectExpr(\"CAST(timestamp AS TIMESTAMP) as timestamp\", \"CAST(value AS STRING) as message\")\n",
    "df = df \\\n",
    "    .withColumn(\"value\", from_json(\"message\", schema)) \\\n",
    "    .select('timestamp', 'value.*')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salman,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "S= 'Salman, Dahir, Salman'\n",
    "try:\n",
    "    d=re.search('^[\\w ]+,', S)\n",
    "    print (d.group())\n",
    "except:\n",
    "    print (S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [StructField(\"created_at\", StringType()),\n",
    "    StructField(\"message\", StringType())]\n",
    "    )\n",
    "df = df \\\n",
    "    .withColumn(\"value\", from_json(\"message\", schema)) \\\n",
    "    .select('timestamp', 'value.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyowm.weatherapi25.weather.Weather'>\n",
      "***Current Weather***\n",
      "19.6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyowm\n",
    "import json\n",
    "import datetime\n",
    "api_key = '3d2e4262254bb8b94eaf837891e9bc38'    #your API Key here as string\n",
    "owm = pyowm.OWM(api_key).weather_manager()\n",
    "\n",
    "def get_current_weather():\n",
    "    try:    \n",
    "        weather_api = owm.weather_at_place('Los Angeles,')  # give where you need to see the weather\n",
    "        weather_data = weather_api.weather\n",
    "        print(type(weather_data))          # get out data in the mentioned location\n",
    "        print(\"***Current Weather***\")\n",
    "        wdict = weather_data.temperature('celsius')\n",
    "        avg_temp = (wdict['temp_max'] + wdict['temp_min'])/2\n",
    "        print(avg_temp)\n",
    "    except: \n",
    "        print ('null')\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "get_current_weather() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'timestamp': 'Tue Jun 21 16:17:17 +0000 2022',\n",
       "  'created_at': 'Tue Jun 21 16:17:17 +0000 2022',\n",
       "  'message': '⚠️ How many parents have received more than one text message from Texas Childrens Hospital regarding the child COVID vaccine? \\n\\nI’ve spoken with several people who have received over 6 text messages since Tuesday. This seems highly unusual.',\n",
       "  'City': 'None',\n",
       "  'Temp': 'null'},\n",
       " {'timestamp': 'Tue Jun 21 16:17:17 +0000 2022',\n",
       "  'created_at': 'Tue Jun 21 16:17:17 +0000 2022',\n",
       "  'message': \"@DiscordDystopia I started publicly showing my art right before COVID struck but I'd really like to do more live i… https://t.co/lXahOUobDD\",\n",
       "  'City': 'Ohio, US',\n",
       "  'Temp': '23.5'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "[{\n",
    "    \"timestamp\": \"Tue Jun 21 16:17:17 +0000 2022\",\"created_at\": \"Tue Jun 21 16:17:17 +0000 2022\", \"message\": \"⚠️ How many parents have received more than one text message from Texas Childrens Hospital regarding the child COVID vaccine? \\n\\nI’ve spoken with several people who have received over 6 text messages since Tuesday. This seems highly unusual.\", \"City\": \"None\", \"Temp\": \"null\"},\n",
    "{\"timestamp\": \"Tue Jun 21 16:17:17 +0000 2022\", \"created_at\": \"Tue Jun 21 16:17:17 +0000 2022\", \"message\": \"@DiscordDystopia I started publicly showing my art right before COVID struck but I'd really like to do more live i… https://t.co/lXahOUobDD\", \"City\": \"Ohio, US\", \"Temp\": \"23.5\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = source.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpointDir.toString)\\\n",
    "  .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .option(\"keyspace\", ks)\\\n",
    "    .option(\"table\", \"kv\")\\\n",
    "  .outputMode(OutputMode.Update)\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+--------------------+\n",
      "|                City|Temp|     _corrupt_record|          created_at|             message|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+\n",
      "|                null|null|                  [{|                null|                null|\n",
      "|                null|null|\"created_at\": \"Tu...|                null|                null|\n",
      "|abstract.the.orig...|null|                null|Tue Jun 21 16:17:...|@DiscordDystopia ...|\n",
      "|                null|null|                   ]|                null|                null|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+\n",
      "\n",
      "22/06/22 11:09:32 ERROR Executor: Exception in task 0.0 in stage 61.0 (TID 61)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/11/1_98cps925d273zrhlp6wcs40000gn/T/ipykernel_635/3258052198.py\", line 34, in <lambda>\n",
      "TypeError: strptime() argument 1 must be str, not None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/06/22 11:09:32 WARN TaskSetManager: Lost task 0.0 in stage 61.0 (TID 61) (localhost executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/11/1_98cps925d273zrhlp6wcs40000gn/T/ipykernel_635/3258052198.py\", line 34, in <lambda>\n",
      "TypeError: strptime() argument 1 must be str, not None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/06/22 11:09:32 ERROR TaskSetManager: Task 0 in stage 61.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/11/1_98cps925d273zrhlp6wcs40000gn/T/ipykernel_635/3258052198.py\", line 34, in <lambda>\nTypeError: strptime() argument 1 must be str, not None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=31'>32</a>\u001b[0m date_process \u001b[39m=\u001b[39m udf(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=32'>33</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: datetime\u001b[39m.\u001b[39mstrftime(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=33'>34</a>\u001b[0m         datetime\u001b[39m.\u001b[39mstrptime(x,\u001b[39m'\u001b[39m\u001b[39m%a\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mb \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS +0000 \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=34'>35</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=35'>36</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=36'>37</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m\"\u001b[39m, date_process(df\u001b[39m.\u001b[39mcreated_at))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/.ipynb#ch0000010?line=37'>38</a>\u001b[0m df\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/11/1_98cps925d273zrhlp6wcs40000gn/T/ipykernel_635/3258052198.py\", line 34, in <lambda>\nTypeError: strptime() argument 1 must be str, not None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, from_json, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf, from_json, col\n",
    "import findspark\n",
    "from datetime import datetime\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"StreamingApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"created_at\", StringType()),\n",
    "    StructField(\"message\", StringType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"Temp\", FloatType())]\n",
    "    )\n",
    "\n",
    "df = spark.read.json(\"/Users/Salman/Desktop/Github/Kafka-Spark-Twitter-Weather-Streaming/twitter/sample2.json\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(\"value\", from_json(\"message\", schema)) \n",
    "  \n",
    "#print(df)\n",
    "#Changing datetime format\n",
    "\n",
    "date_process = udf(\n",
    "    lambda x: datetime.strftime(\n",
    "        datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "    )\n",
    "df = df.withColumn(\"created_at\", date_process(df.created_at))\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
